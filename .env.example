# Mistral Reproducibility - Environment Configuration
# Copy this file to .env and customize for your environment

# ============================================================================
# Model Configuration
# ============================================================================
# Mistral model from HuggingFace (22B parameter model)
MISTRAL_MODEL=mistralai/Mistral-Small-3.2-24B-Instruct-2506

# Path to SAE (Sparse Autoencoder) weights inside container
SAE_PATH=/models/sae_mistral_l30.pt

# Model cache directory for HuggingFace downloads
MODEL_CACHE_DIR=/root/.cache/huggingface

# Model precision (bfloat16 recommended for A100, float16 for older GPUs)
MODEL_DTYPE=bfloat16

# Device to use (cuda for GPU, cpu for CPU-only)
DEVICE=cuda

# ============================================================================
# Service Ports
# ============================================================================
# Backend API port (FastAPI)
BACKEND_PORT=8000

# Frontend port (SvelteKit)
FRONTEND_PORT=3000

# ============================================================================
# Hardware Configuration
# ============================================================================
# GPU selection (comma-separated indices, e.g., "0" or "0,1,2,3")
CUDA_VISIBLE_DEVICES=0

# Maximum GPU memory to allocate in GB
# Standard GPU (24GB): 20-24
# A100 80GB: 70-80
MAX_MEMORY_GB=24

# Enable torch.compile optimization (experimental, DGX recommended)
TORCH_COMPILE=0

# ============================================================================
# Optional: HuggingFace Token
# ============================================================================
# Required only if downloading gated models
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=

# ============================================================================
# Optional: Local Model Paths
# ============================================================================
# Mount pre-downloaded models from host to avoid re-downloading
# Example: /mnt/models or /data/models
LOCAL_MODELS_DIR=./models

# DGX-specific: Path to shared model storage
# DGX_MODEL_PATH=/data/models

# ============================================================================
# Logging
# ============================================================================
# Log level: debug, info, warning, error
LOG_LEVEL=info
